import os
import random

import pandas as pd
import numpy as np

from PIL import Image
from tqdm import tqdm

from sklearn.model_selection import train_test_split

import torch
from torch.utils.data import Dataset, DataLoader, Subset
import torchvision.models as models
import torchvision.transforms as transforms
import torch.nn.functional as F
from torch import nn, optim

from sklearn.metrics import log_loss


import config
from util.dataloader import ImageDataset
from util.model import ResNet152, SwinArcClassifier
import wandb

wandb.init(
    project="car-classification",
    config={
        "epochs": config.epochs,
        "batch_size": config.batch_size,
        "learning_rate": config.lr,
        "model": "SwinArcClassifier",
        "backbone": "swin_base_patch4_window7_224",
        "margin": 0.3,
        "scale": 30.0,
        "emb_dim": 512
    }
)
cfg = wandb.config

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

train_transform = transforms.Compose([
    transforms.Resize((config.img_size, config.img_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

val_transform = transforms.Compose([
    transforms.Resize((config.img_size, config.img_size)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

dataset = ImageDataset(config.train_root, transform=None)
print(f"총이미지수 {len(dataset.samples) = }")

target = [label for _, label in dataset.samples]
class_name = dataset.classes

train_index, val_index = train_test_split(range(len(target)), test_size=0.2, stratify=target, random_state=config.seed)

train_dataset = Subset(ImageDataset(config.train_root, transform=train_transform), indices=train_index)
val_dataset = Subset(ImageDataset(config.train_root, transform=val_transform), indices=val_index)


train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)






#model = ResNet152(num_classes=len(class_name)).to(device)
model = SwinArcClassifier(num_classes=len(class_name), pretrained=True).to(device)

criterion = nn.CrossEntropyLoss()

optimizer = optim.AdamW(model.parameters(), lr=config.lr)

scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=config.epochs, eta_min=1e-6)

best_logloss = float('inf')



for epoch in range(1, config.epochs + 1):
    # ==== Training ====
    model.train()
    train_loss = 0.0
    for images, labels in tqdm(train_loader, desc=f"Epoch {epoch}/{config.epochs} - Training"):
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()
        # ArcFace margin 적용 로짓 반환
        logits, _ = model(images, labels)
        loss = criterion(logits, labels)
        loss.backward()
        optimizer.step()
        train_loss += loss.item() * images.size(0)

    avg_train_loss = train_loss / len(train_loader.dataset)

    # 스케줄러 스텝 (epoch 기준)
    scheduler.step()

    # ==== Validation ====
    model.eval()
    val_loss = 0.0
    correct = 0
    total = 0
    all_probs = []
    all_labels = []

    with torch.no_grad():
        for images, labels in tqdm(val_loader, desc=f"Epoch {epoch}/{config.epochs} - Validation"):
            images, labels = images.to(device), labels.to(device)
            # inference 모드: margin 없이 호출
            logits = model(images)
            loss = criterion(logits, labels)
            val_loss += loss.item() * images.size(0)

            preds = torch.argmax(logits, dim=1)
            correct += (preds == labels).sum().item()
            total += labels.size(0)

            probs = F.softmax(logits, dim=1)
            all_probs.extend(probs.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_val_loss = val_loss / len(val_loader.dataset)
    val_acc = 100.0 * correct / total
    val_logloss = log_loss(all_labels, all_probs, labels=list(range(len(class_name))))

    wandb.log({
        "epoch": epoch,
        "train_loss": avg_train_loss,
        "val_loss": avg_val_loss,
        "val_accuracy": val_acc,
        "val_logloss": val_logloss,
        "learning_rate": scheduler.get_last_lr()[0]
    })


    print(
        f"Epoch {epoch}/{config.epochs} | "
        f"Train Loss: {avg_train_loss:.4f} | "
        f"Val Loss: {avg_val_loss:.4f} | "
        f"Val Acc: {val_acc:.2f}% | "
        f"Val LogLoss: {val_logloss:.4f}"
    )

    # 체크포인트 저장
    if val_logloss < best_logloss:
        best_logloss = val_logloss
        torch.save(model.state_dict(), "./models/SwinArcClassifier.pth")
        wandb.run.summary["best_val_logloss"] = val_logloss
        wandb.run.summary["best_epoch"] = epoch
        print(f"[Checkpoint] Saved best model at epoch {epoch}, LogLoss: {val_logloss:.4f}")

# 최종 학습률 확인
print(f"최종 학습률: {scheduler.get_last_lr()[0]:.2e}")
wandb.log({"final_learning_rate": scheduler.get_last_lr()[0]})
wandb.finish()